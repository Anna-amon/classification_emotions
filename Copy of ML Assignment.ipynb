{"cells":[{"cell_type":"markdown","metadata":{"id":"an85JtWzt1ty"},"source":["Emotion Recognition Classification Task. My Emotion Data Set:\n","\n","The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n","\n","train.csv contains two columns, \"emotion\" and \"pixels\". The \"emotion\" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image. The \"pixels\" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. test.csv contains only the \"pixels\" column and your task is to predict the emotion column.\n","\n","The training set consists of 28,709 examples. The public test set used for the leaderboard consists of 3,589 examples. The final test set, which was used to determine the winner of the competition, consists of another 3,589 examples.\n","\n","- What did you learn from exploring the data that will influence how you approach modelling this dataset?\n","- How did you process/pre-process the data (note it may be different for different models)\n","- Data Augmentation: You don’t have that much data, employ different data augmentation strategies, to help your training.\n","- Standard ML Baseline: Create a simple baseline of your choice using standard ML\n","- Deep NN models: Create a deep NN, and find a good configuration (explore at least 5-10 configurations)\n","- Complex NN Models: Rather than employing a standard deep NN model, this is your chance to use more sophisticated architectures. For example, CNNs, ResNet, etc.  to improve performance.\n","\n","Try to find interesting architectures that might help improve the performance. They must be different architectures (not just extra layers). Include the mean and standard deviations. Include the results with and without your different training schedules (i.e. with/without different data augmentation)\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1709998838686,"user":{"displayName":"anna nash","userId":"01376099043001282372"},"user_tz":0},"id":"zctH-KUft1t2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: keras-tuner in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (1.4.7)\n","Requirement already satisfied: keras in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras-tuner) (3.0.5)\n","Requirement already satisfied: packaging in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras-tuner) (23.2)\n","Requirement already satisfied: requests in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras-tuner) (2.31.0)\n","Requirement already satisfied: kt-legacy in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras-tuner) (1.0.5)\n","Requirement already satisfied: absl-py in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras->keras-tuner) (2.1.0)\n","Requirement already satisfied: numpy in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras->keras-tuner) (1.26.4)\n","Requirement already satisfied: rich in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras->keras-tuner) (13.7.1)\n","Requirement already satisfied: namex in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras->keras-tuner) (0.0.7)\n","Requirement already satisfied: h5py in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras->keras-tuner) (3.10.0)\n","Requirement already satisfied: dm-tree in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras->keras-tuner) (0.1.8)\n","Requirement already satisfied: ml-dtypes in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from keras->keras-tuner) (0.3.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from requests->keras-tuner) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from requests->keras-tuner) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from requests->keras-tuner) (2024.2.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from rich->keras->keras-tuner) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from rich->keras->keras-tuner) (2.17.2)\n","Requirement already satisfied: mdurl~=0.1 in c:\\users\\annal\\anaconda3\\envs\\machine_learning\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"]}],"source":["!pip install keras-tuner\n","import pandas as pd\n","import sklearn as sklearn\n","from sklearn.linear_model import Perceptron\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","import keras_tuner as kt\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2764,"status":"ok","timestamp":1710002098127,"user":{"displayName":"anna nash","userId":"01376099043001282372"},"user_tz":0},"id":"U9wkxP1z7xjg"},"outputs":[],"source":["data = pd.read_csv('my_emotion_train.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1710002099735,"user":{"displayName":"anna nash","userId":"01376099043001282372"},"user_tz":0},"id":"vvIblTyzt1t5","outputId":"614f8b31-bf35-470f-ac9d-56610377ed8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of the dataset: (29000, 3)\n","First few rows of the dataset:\n","      id  emotion                                             pixels\n","0   9415        6  29 16 18 18 18 20 19 18 17 17 17 18 17 18 17 1...\n","1  19109        3  126 154 167 181 188 194 195 194 196 195 198 20...\n","2  21523        2  169 220 218 208 184 144 72 73 143 183 203 210 ...\n","3   2076        3  60 64 72 80 83 83 80 82 89 106 114 125 125 127...\n","4  13957        3  174 148 121 97 78 70 62 57 54 54 42 58 40 64 1...\n","Summary statistics for the dataset:\n","                  id       emotion  \\\n","count   29000.000000  29000.000000   \n","unique           NaN           NaN   \n","top              NaN           NaN   \n","freq             NaN           NaN   \n","mean    17909.302759      3.321828   \n","std     10363.556419      1.871095   \n","min         0.000000      0.000000   \n","25%      8922.500000      2.000000   \n","50%     17911.500000      3.000000   \n","75%     26869.250000      5.000000   \n","max     35885.000000      6.000000   \n","\n","                                                   pixels  \n","count                                               29000  \n","unique                                              27752  \n","top     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n","freq                                                   11  \n","mean                                                  NaN  \n","std                                                   NaN  \n","min                                                   NaN  \n","25%                                                   NaN  \n","50%                                                   NaN  \n","75%                                                   NaN  \n","max                                                   NaN  \n","Missing values in the dataset:\n","id         0\n","emotion    0\n","pixels     0\n","dtype: int64\n"]}],"source":["# Shape of the dataset\n","# Display the first few rows\n","# Summary statistics for the dataset\n","# Checking for missing values\n","\n","print(\"Shape of the dataset:\", data.shape)\n","\n","print(\"First few rows of the dataset:\")\n","print(data.head())\n","\n","print(\"Summary statistics for the dataset:\")\n","print(data.describe(include='all'))\n","\n","print(\"Missing values in the dataset:\")\n","print(data.isnull().sum())"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710002103868,"user":{"displayName":"anna nash","userId":"01376099043001282372"},"user_tz":0},"id":"YRU7OrUEt1t6"},"outputs":[],"source":["# Separate the target variable ('emotion') and predictors ('pixels')\n","# At this point, 'X' contains only the 'pixels' data, and 'y' contains the 'emotion' labels.\n","# keep the 'ID' column for later reference\n","\n","X = data['pixels'].copy()\n","y = data['emotion'].copy()\n","id = data['id'].copy()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2511,"status":"ok","timestamp":1710002108395,"user":{"displayName":"anna nash","userId":"01376099043001282372"},"user_tz":0},"id":"doO4u5MKt1t7"},"outputs":[],"source":["\n","# Convert 'pixels' from a string to a 2D numpy array and normalize the pixel values\n","data['pixels'] = data['pixels'].apply(lambda x: np.fromstring(x, dtype=int, sep=' ').reshape(48, 48) / 255.0)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710002108396,"user":{"displayName":"anna nash","userId":"01376099043001282372"},"user_tz":0},"id":"si5-gaOot1t8"},"outputs":[],"source":["# Now, create a 4D array (for TensorFlow) where each image has shape (48, 48, 1) indicating height, width, and channel\n","X = np.stack(data['pixels'].values).reshape(-1, 48, 48, 1)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":537,"status":"ok","timestamp":1710002111079,"user":{"displayName":"anna nash","userId":"01376099043001282372"},"user_tz":0},"id":"wUeQgC5mt1t8","outputId":"66246d59-60e0-486f-b9d9-584817bb7b9c"},"outputs":[{"data":{"text/plain":["(29000, 48, 48, 1)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["X.shape"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1710002111462,"user":{"displayName":"anna nash","userId":"01376099043001282372"},"user_tz":0},"id":"NaRta8Uqt1t9","outputId":"90e9c3bf-11c5-44e9-c5f8-4d36a1a769b8"},"outputs":[{"data":{"text/plain":["(29000,)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["y.shape"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":476,"status":"ok","timestamp":1710002113388,"user":{"displayName":"anna nash","userId":"01376099043001282372"},"user_tz":0},"id":"qhJYa2Hbt1t-","outputId":"4809723a-0e97-4abf-e68b-00f68f1b1b0c"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# To see what the image that we are classifying looks like\n","image = data['pixels'].iloc[0]\n","plt.imshow(image, cmap='gray')\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Assuming X and y are defined\n","\n","# Example for image data: Scale each channel individually\n","scaler = StandardScaler()\n","\n","# Assume X is shaped as (samples, height, width, channels)\n","# Reshape to (samples * height * width, channels) for scaling\n","X_reshaped = X.reshape(-1, X.shape[-1])\n","X_scaled_reshaped = scaler.fit_transform(X_reshaped)\n","\n","# Reshape back to original shape\n","X = X_scaled_reshaped.reshape(X.shape)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Define the proportion of the data to use for validation\n","val_size = 0.2  # For example, 20% of the data will be used for validation\n","\n","# Split the dataset into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=42)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# New Model - For the training set: Data augmentation\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,  # Rescale pixel values\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","# For the validation set: Only rescaling\n","validation_datagen = ImageDataGenerator(\n","    rescale=1./255  # It's important to rescale the same way as training data\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Apply the data augmentation to the training set and only rescaling to the validation set\n","train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n","validation_generator = validation_datagen.flow(X_val, y_val, batch_size=32)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class AugmentedBatchNormModel(keras.Model):\n","    def __init__(self, input_shape, num_classes, units, activation=\"relu\", dropout_rate=0.5, **kwargs):\n","        super().__init__(**kwargs)\n","        self.dense1 = keras.layers.Dense(units, use_bias=False)\n","        self.bn1 = keras.layers.BatchNormalization()\n","        self.act1 = keras.layers.Activation(activation)\n","        self.dropout1 = keras.layers.Dropout(dropout_rate)  # Add dropout layer\n","        self.dense2 = keras.layers.Dense(units, use_bias=False)\n","        self.bn2 = keras.layers.BatchNormalization()\n","        self.act2 = keras.layers.Activation(activation)\n","        self.dropout2 = keras.layers.Dropout(dropout_rate)  # Add dropout layer\n","        self.flatten = keras.layers.Flatten()\n","        self.output_layer = keras.layers.Dense(num_classes, activation='softmax')\n","\n","    def call(self, inputs, training=False):\n","        x = self.dense1(inputs)\n","        x = self.bn1(x)\n","        x = self.act1(x)\n","        x = self.dropout1(x, training=training)  # Apply dropout only during training\n","        x = self.dense2(x)\n","        x = self.bn2(x)\n","        x = self.act2(x)\n","        x = self.dropout2(x, training=training)  # Apply dropout only during training\n","        x = self.flatten(x)\n","        return self.output_layer(x)\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Define input_shape and num_classes globally\n","input_shape = X_train.shape[1:]  # This assumes X_train is your input features numpy array (val has same shape so no need to specify)\n","num_classes = np.unique(y_train).size  # This assumes y_train is your target labels numpy array\n","\n","def model_builder(hp):\n","    units = hp.Int('units', min_value=10, max_value=100, step=10)\n","    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","    optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd'])\n","    activation_choice = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])\n","    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n","\n","    model = AugmentedBatchNormModel(input_shape=input_shape, num_classes=num_classes, units=units, activation=activation_choice, dropout_rate=dropout_rate)\n","\n","    if optimizer_choice == 'adam':\n","        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","    else:\n","        optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n","\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 3 Complete [1d 05h 03m 27s]\n","val_accuracy: 0.3832758665084839\n","\n","Best val_accuracy So Far: 0.3832758665084839\n","Total elapsed time: 1d 06h 01m 49s\n","\n","Search: Running Trial #4\n","\n","Value             |Best Value So Far |Hyperparameter\n","100               |70                |units\n","0.0001            |0.01              |learning_rate\n","adam              |sgd               |optimizer\n","relu              |sigmoid           |activation\n","0.3               |0                 |dropout_rate\n","\n","Epoch 1/20\n","\u001b[1m725/725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 451ms/step - accuracy: 0.2529 - loss: 2.7505 - val_accuracy: 0.2636 - val_loss: 2.0824\n","Epoch 2/20\n","\u001b[1m725/725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 442ms/step - accuracy: 0.3055 - loss: 2.2188 - val_accuracy: 0.2907 - val_loss: 1.8646\n","Epoch 3/20\n","\u001b[1m725/725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 416ms/step - accuracy: 0.3220 - loss: 2.1617 - val_accuracy: 0.3197 - val_loss: 1.8380\n","Epoch 4/20\n","\u001b[1m725/725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34752s\u001b[0m 48s/step - accuracy: 0.3408 - loss: 2.0574 - val_accuracy: 0.2976 - val_loss: 1.8628\n","Epoch 5/20\n","\u001b[1m725/725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 457ms/step - accuracy: 0.3529 - loss: 1.9372 - val_accuracy: 0.3255 - val_loss: 1.8835\n","Epoch 6/20\n","\u001b[1m381/725\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2:35\u001b[0m 453ms/step - accuracy: 0.3601 - loss: 1.9364"]}],"source":["# Initiate the search process\n","tuner = kt.RandomSearch(\n","    model_builder,\n","    objective='val_accuracy',\n","    max_trials=10,\n","    executions_per_trial=1,\n","    directory='keras_tuner_dir',\n","    project_name='augmented_batchnorm_model_opt'\n",")\n","\n","# Initiate the search process\n","tuner.search(\n","    x=X_train,\n","    y=y_train,\n","    epochs=20,\n","    validation_data=(X_val, y_val)\n",")\n","\n","# After the search is complete, retrieve the best hyperparameters\n","augmented_best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","# Now, you can safely print or use best_hps since it's defined\n","print(f\"\"\"\n","The hyperparameter search is complete. The optimal number of units is {best_hps.get('units')},\n","the optimal learning rate for the optimizer is {best_hps.get('learning_rate')},\n","and the best optimizer is {best_hps.get('optimizer')},\n","The best activation function is {best_hps.get('activation')},\n","The best dropout rate is {best_hps.get('dropout_rate')}.\n","\"\"\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1iGnqsg--pn9rA0Pl9r36R_Agi8pvIcol","timestamp":1710003733643}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
