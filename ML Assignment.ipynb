{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotion Recognition Classification Task. My Emotion Data Set:\n",
    "\n",
    "The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
    "\n",
    "train.csv contains two columns, \"emotion\" and \"pixels\". The \"emotion\" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image. The \"pixels\" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. test.csv contains only the \"pixels\" column and your task is to predict the emotion column.\n",
    "\n",
    "The training set consists of 28,709 examples. The public test set used for the leaderboard consists of 3,589 examples. The final test set, which was used to determine the winner of the competition, consists of another 3,589 examples.\n",
    "\n",
    "- What did you learn from exploring the data that will influence how you approach modelling this dataset?\n",
    "- How did you process/pre-process the data (note it may be different for different models)\n",
    "- Data Augmentation: You don’t have that much data, employ different data augmentation strategies, to help your training.\n",
    "- Standard ML Baseline: Create a simple baseline of your choice using standard ML\n",
    "- Deep NN models: Create a deep NN, and find a good configuration (explore at least 5-10 configurations)\n",
    "- Complex NN Models: Rather than employing a standard deep NN model, this is your chance to use more sophisticated architectures. For example, CNNs, ResNet, etc.  to improve performance.\n",
    "\n",
    "Try to find interesting architectures that might help improve the performance. They must be different architectures (not just extra layers). Include the mean and standard deviations. Include the results with and without your different training schedules (i.e. with/without different data augmentation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sklearn\n",
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('C:\\\\Users\\\\annal\\\\OneDrive\\\\Dokumente\\\\Year Two\\\\Machine Learning\\\\my_emotion_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (29000, 3)\n",
      "First few rows of the dataset:\n",
      "      id  emotion                                             pixels\n",
      "0   9415        6  29 16 18 18 18 20 19 18 17 17 17 18 17 18 17 1...\n",
      "1  19109        3  126 154 167 181 188 194 195 194 196 195 198 20...\n",
      "2  21523        2  169 220 218 208 184 144 72 73 143 183 203 210 ...\n",
      "3   2076        3  60 64 72 80 83 83 80 82 89 106 114 125 125 127...\n",
      "4  13957        3  174 148 121 97 78 70 62 57 54 54 42 58 40 64 1...\n",
      "Summary statistics for the dataset:\n",
      "                  id       emotion  \\\n",
      "count   29000.000000  29000.000000   \n",
      "unique           NaN           NaN   \n",
      "top              NaN           NaN   \n",
      "freq             NaN           NaN   \n",
      "mean    17909.302759      3.321828   \n",
      "std     10363.556419      1.871095   \n",
      "min         0.000000      0.000000   \n",
      "25%      8922.500000      2.000000   \n",
      "50%     17911.500000      3.000000   \n",
      "75%     26869.250000      5.000000   \n",
      "max     35885.000000      6.000000   \n",
      "\n",
      "                                                   pixels  \n",
      "count                                               29000  \n",
      "unique                                              27752  \n",
      "top     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
      "freq                                                   11  \n",
      "mean                                                  NaN  \n",
      "std                                                   NaN  \n",
      "min                                                   NaN  \n",
      "25%                                                   NaN  \n",
      "50%                                                   NaN  \n",
      "75%                                                   NaN  \n",
      "max                                                   NaN  \n",
      "Missing values in the dataset:\n",
      "id         0\n",
      "emotion    0\n",
      "pixels     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape of the dataset\n",
    "# Display the first few rows\n",
    "# Summary statistics for the dataset\n",
    "# Checking for missing values\n",
    "\n",
    "print(\"Shape of the dataset:\", data.shape)\n",
    "\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"Summary statistics for the dataset:\")\n",
    "print(data.describe(include='all'))\n",
    "\n",
    "print(\"Missing values in the dataset:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable ('emotion') and predictors ('pixels')\n",
    "# At this point, 'X' contains only the 'pixels' data, and 'y' contains the 'emotion' labels.\n",
    "# keep the 'ID' column for later reference\n",
    "\n",
    "X = data['pixels'].copy()\n",
    "y = data['emotion'].copy()\n",
    "id = data['id'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert 'pixels' from a string to a 2D numpy array and normalize the pixel values\n",
    "data['pixels'] = data['pixels'].apply(lambda x: np.fromstring(x, dtype=int, sep=' ').reshape(48, 48) / 255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a 4D array (for TensorFlow) where each image has shape (48, 48, 1) indicating height, width, and channel\n",
    "X = np.stack(data['pixels'].values).reshape(-1, 48, 48, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29000, 48, 48, 1)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29000,)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To see what the image that we are classifying looks like\n",
    "image = data['pixels'].iloc[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build, train, and evaluate a simple Wide & Deep neural network model using TensorFlow's Keras API. Creating a class which will then construct and instantiate a model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we need to consider how we would like to adjust our labels for the target variable. We can either:\n",
    "# a) one hot encode the Labels as it is multi-class classification, as each instance can only belong to one class, or;\n",
    "# a) we can use sparse labels, which is more efficient for computational time, but some activation functions are not compatable with this, where as they may be with one hot encoded labels\n",
    "# If we decide to encode, use: y = tf.keras.utils.to_categorical(y, num_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Creating a class which will then construct and instantiate a model. DNN incorporating Batch Normalisation\n",
    "# Dense layers often have a lot of parameters. This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of\n",
    "# overfitting, especially when you do not have a lot of training data. \n",
    "\n",
    "# Example configurations to explore, now including optimizers in the configurations\n",
    "configurations = [\n",
    "    {'units': 30, 'activation': 'relu', 'epochs': 20, 'optimizer': keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9)}\n",
    "]\n",
    "\n",
    "class BatchNormModel(keras.Model):\n",
    "    def __init__(self, input_shape, num_classes, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.norm_layer = keras.layers.Normalization(axis=-1)\n",
    "        self.dense1 = keras.layers.Dense(units, use_bias=False)\n",
    "        self.bn1 = keras.layers.BatchNormalization()\n",
    "        self.act1 = keras.layers.Activation(activation)\n",
    "        self.dense2 = keras.layers.Dense(units, use_bias=False)\n",
    "        self.bn2 = keras.layers.BatchNormalization()\n",
    "        self.act2 = keras.layers.Activation(activation)\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.output_layer = keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.norm_layer(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "def run_config(X, y, config, val_size=0.2):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=42)\n",
    "    \n",
    "    # Create a new model instance with the current configuration\n",
    "    model = BatchNormModel(input_shape=X_train.shape[1:], num_classes=np.unique(y).size, units=config['units'], activation=config['activation'])\n",
    "    \n",
    "    # Use the specified optimizer from the configuration\n",
    "    model.compile(optimizer=config['optimizer'], loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Adapt normalization layer\n",
    "    model.norm_layer.adapt(X_train)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=config['epochs'], validation_data=(X_val, y_val), verbose=2)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run through each configuration\n",
    "for config in configurations:\n",
    "    history = run_config(X, y, config)\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"Configuration: {config}, Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and y are defined\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example for image data: Scale each channel individually\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Assume X is shaped as (samples, height, width, channels)\n",
    "# Reshape to (samples * height * width, channels) for scaling\n",
    "X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "X_scaled_reshaped = scaler.fit_transform(X_reshaped)\n",
    "\n",
    "# Reshape back to original shape\n",
    "X = X_scaled_reshaped.reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "class BatchNormModel(keras.Model):\n",
    "    def __init__(self, input_shape, num_classes, units, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Removed the normalization layer\n",
    "        self.dense1 = keras.layers.Dense(units, use_bias=False)\n",
    "        self.bn1 = keras.layers.BatchNormalization()\n",
    "        self.act1 = keras.layers.Activation(activation)\n",
    "        self.dense2 = keras.layers.Dense(units, use_bias=False)\n",
    "        self.bn2 = keras.layers.BatchNormalization()\n",
    "        self.act2 = keras.layers.Activation(activation)\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.output_layer = keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)  # Input directly to dense1, normalization moved to preprocessing\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input_shape and num_classes globally\n",
    "input_shape = X_train.shape[1:]  # This assumes X_train is your input features numpy array\n",
    "num_classes = np.unique(y_train).size  # This assumes y_train is your target labels numpy array\n",
    "\n",
    "def model_builder(hp):\n",
    "    units = hp.Int('units', min_value=10, max_value=100, step=10)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
    "\n",
    "    # Assuming input_shape and num_classes are defined globally or passed in some way\n",
    "    model = BatchNormModel(input_shape=input_shape, num_classes=num_classes, units=units, activation='relu')\n",
    "    \n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras_tuner_dir\\batchnorm_model_opt\\tuner0.json\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "100               |30                |units\n",
      "0.001             |0.0001            |learning_rate\n",
      "adam              |adam              |optimizer\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m725/725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 210ms/step - accuracy: 0.2891 - loss: 1.9362 - val_accuracy: 0.3619 - val_loss: 1.6507\n",
      "Epoch 2/20\n",
      "\u001b[1m725/725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 209ms/step - accuracy: 0.3788 - loss: 1.5976 - val_accuracy: 0.3710 - val_loss: 1.6280\n",
      "Epoch 3/20\n",
      "\u001b[1m725/725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 208ms/step - accuracy: 0.4375 - loss: 1.4818 - val_accuracy: 0.3674 - val_loss: 1.6528\n",
      "Epoch 4/20\n",
      "\u001b[1m131/725\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 191ms/step - accuracy: 0.5040 - loss: 1.3323"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 17\u001b[0m\n\u001b[0;32m      7\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[0;32m      8\u001b[0m     model_builder,\n\u001b[0;32m      9\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatchnorm_model_opt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Initiate the search process\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# After the search is complete, retrieve the best hyperparameters\u001b[39;00m\n\u001b[0;32m     20\u001b[0m best_hps \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:323\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 323\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[0;32m    325\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    326\u001b[0m     )\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\annal\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the proportion of the data to use for validation\n",
    "val_size = 0.2  # For example, 20% of the data will be used for validation\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=42)\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='batchnorm_model_opt'\n",
    ")\n",
    "\n",
    "# Initiate the search process\n",
    "tuner.search(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
    "\n",
    "# After the search is complete, retrieve the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Now, you can safely print or use best_hps since it's defined\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units is {best_hps.get('units')}, \n",
    "the optimal learning rate for the optimizer is {best_hps.get('learning_rate')}, \n",
    "and the best optimizer is {best_hps.get('optimizer')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the best model using optimal hyperparameters\n",
    "best_model = BatchNormModel(input_shape=input_shape, num_classes=num_classes, \n",
    "                            units=best_hps.get('units'), activation='relu')\n",
    "\n",
    "# Compile the best model\n",
    "if best_hps.get('optimizer') == 'adam':\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate'))\n",
    "else:\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=best_hps.get('learning_rate'), momentum=0.9)\n",
    "\n",
    "best_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the best model\n",
    "history = best_model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the best model\n",
    "val_loss, val_accuracy = best_model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
